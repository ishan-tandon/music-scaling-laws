# Scaling Laws for Symbolic Music Generation
**A systematic study of neural scaling laws applied to symbolic music (ABC notation), training Transformer models from 1 Million to 738 Million parameters.**

## Overview
This project investigates whether symbolic music generation adheres to the same power-law scaling dynamics observed in natural language processing (Kaplan et al., 2020). By training a suite of Decoder-Only Transformers (GPT) and LSTM baselines on the **LAKH** dataset, we demonstrate that:

1. **Music follows Power Laws:** Test loss decreases linearly with model size on a log-log scale (L \propto N^{-\alpha}).
2. **No Saturation:** Even at **738M parameters (XXL)**, the model shows no signs of diminishing returns.
3. **Architecture Matters:** Under iso-compute conditions (98M tokens), Transformers significantly outperform LSTMs, which plateau early (~25M params).

---

## Key Findings| Model Size | Parameters | Test Perplexity | Emergent Capabilities |
| --- | --- | --- | --- |
| **Tiny** | 1M | ~2.10 | Valid ABC characters, basic delimiters. |
| **Small** | 6M | ~1.75 | Local syntax, bar lines, repeat signs. |
| **Medium** | 25M | ~1.45 | Key signatures, consistent time signatures. |
| **Large** | 85M | ~1.38 | Harmonic consistency, basic phrasing. |
| **XXL** | **738M** | **1.417** | **Global structure, polyphony, genre stylization.** |

---

## Installation & SetupWe strongly recommend using **Docker** to ensure a reproducible environment with the correct CUDA drivers.

### Prerequisites* NVIDIA GPU (RTX 3090/4090 recommended for XXL training).
* Docker & Docker Compose.

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/music-scaling-laws.git
cd music-scaling-laws

```

### 2. Build the Container
```bash
docker compose up -d --build

```

### 3. Enter the Development Environment
```bash
docker compose exec dev bash

```

---

## Quick Start
### 1. Data PreparationPlace your `.abc` files in `data/raw/`. The script will tokenize them into binary files for training.

```bash
python src/prepare.py --input_path data/raw --output_dir data/processed

```

###2. Train a "Tiny" Debug ModelVerify the pipeline works by training a 1M parameter model for a few steps.

```bash
python src/train.py \
    --model_name debug_tiny \
    --n_layer 2 --n_embd 256 --n_head 4 \
    --max_iters 500 --batch_size 64

```

### 3. Generate Music (Sampling)Generate a sample from your trained model.

```bash
python src/sample.py \
    --checkpoint_dir checkpoints/debug_tiny \
    --prompt "M:4/4\nK:C\n" \
    --num_samples 5

```

---

## Reproducing the Scaling Study###Transformer Scaling SuiteTo reproduce the scaling curve, run the following configurations.

**Small (6M):**

```bash
python src/train.py --model_name gpt_small --n_layer 6 --n_embd 384 --n_head 6

```

**Medium (25M):**

```bash
python src/train.py --model_name gpt_medium --n_layer 12 --n_embd 576 --n_head 12

```

**The "XXL" Model (738M Params):**
*Optimized for RTX 3090 (24GB VRAM).*

```bash
python src/train.py \
    --model_name gpt_xxl \
    --n_layer 24 --n_embd 1600 --n_head 25 \
    --batch_size 1 --grad_accum 32 \
    --max_iters 10000

```

### RNN Baseline ComparisonTo run the iso-compute comparison (matching total tokens seen):

```bash
# Example: RNN Large (87M)
python src/train_rnn.py \
    --model_name rnn_large \
    --n_layer 5 --n_embd 1472 \
    --context_size 128 \
    --max_iters 3000

```

---

## EvaluationCalculate the formal **Perplexity** on the held-out test set.

```bash
python src/calc_perplexity.py --checkpoint checkpoints/gpt_xxl/best.pt

```

**Syntactic Validity Check:**
Parses generated files to check for ABC syntax errors and MIDI compilability.

```bash
python src/eval_syntax.py --folder output/samples/

```

---

## Sample OutputsGenerated by the **XXL (738M)** model:

* **"Heavy Metal Gallop" (Genre Stylization):** Correctly inferred 1/16th notes and E Minor key; generated triplet "gallop" rhythms.
* **"Classic Rock Riff" (Polyphony):** Successfully generated multi-voice ABC (`V:1`, `V:2`) and assigned correct MIDI instrument codes (Bass Guitar).
* **"Blues Shuffle" (Theory):** Automatically selected 12/8 meter for a "Shuffle" title and utilized "Blue Notes" (Minor 3rd over Major Key).

---

## Acknowledgments
* Based on *Kaplan et al., Scaling Laws for Neural Language Models (2020)*.
* Built with [nanoGPT](https://github.com/karpathy/nanoGPT) and PyTorch.
