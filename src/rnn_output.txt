rohan@jsba:~/music-scaling-laws$ docker compose exec dev bash src/05_run_rnn.sh
WARN[0000] /home/rohan/music-scaling-laws/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion
================================================================
           STARTING RNN SCALING STUDY (100M TOKENS)
================================================================
[1/4] Training RNN TINY...
Model Parameters: 1.10M
Training RNN rnn_tiny...
Eff. Batch: 256 (Physical 32 * Accum 8)
step 0: train loss 4.5915, val loss 4.5949
step 0 | loss 4.5917
step 10 | loss 3.2548
step 20 | loss 3.1203
step 30 | loss 3.0692
step 40 | loss 3.0706
step 50 | loss 3.0537
step 60 | loss 3.0003
step 70 | loss 3.0508
step 80 | loss 2.9560
step 90 | loss 2.9721
step 100 | loss 3.0367
step 110 | loss 2.9469
step 120 | loss 2.9290
step 130 | loss 2.8833
step 140 | loss 2.7606
step 150 | loss 2.5801
step 160 | loss 2.3783
step 170 | loss 2.1851
step 180 | loss 2.1915
step 190 | loss 2.1890
step 200 | loss 1.9333
step 210 | loss 1.7039
step 220 | loss 1.7260
step 230 | loss 1.8333
step 240 | loss 1.6280
step 250: train loss 1.5109, val loss 2.4649
step 250 | loss 1.6120
step 260 | loss 1.4584
step 270 | loss 1.2247
step 280 | loss 1.1397
step 290 | loss 1.3205
step 300 | loss 1.4356
step 310 | loss 1.1743
step 320 | loss 1.2972
step 330 | loss 1.3500
step 340 | loss 0.9921
step 350 | loss 1.0315
step 360 | loss 0.8991
step 370 | loss 1.0442
step 380 | loss 0.8944
step 390 | loss 0.9561
step 400 | loss 0.6808
step 410 | loss 0.9458
step 420 | loss 0.6151
step 430 | loss 0.8396
step 440 | loss 0.9174
step 450 | loss 0.8213
step 460 | loss 0.7312
step 470 | loss 0.8784
step 480 | loss 0.9117
step 490 | loss 0.5968
step 500: train loss 0.6548, val loss 1.4949
step 500 | loss 0.9440
step 510 | loss 0.5802
step 520 | loss 0.7964
step 530 | loss 0.6373
step 540 | loss 0.6296
step 550 | loss 0.6921
step 560 | loss 0.6303
step 570 | loss 0.5870
step 580 | loss 0.6164
step 590 | loss 0.6084
step 600 | loss 0.6806
step 610 | loss 0.3846
step 620 | loss 0.4731
step 630 | loss 0.5800
step 640 | loss 0.5800
step 650 | loss 0.5693
step 660 | loss 0.5205
step 670 | loss 0.4904
step 680 | loss 0.5087
step 690 | loss 0.5151
step 700 | loss 0.5564
step 710 | loss 0.4706
step 720 | loss 0.4637
step 730 | loss 0.5341
step 740 | loss 0.5476
step 750: train loss 0.5271, val loss 1.2745
step 750 | loss 0.5613
step 760 | loss 0.4606
step 770 | loss 0.6032
step 780 | loss 0.4487
step 790 | loss 0.4626
step 800 | loss 0.4629
step 810 | loss 0.5425
step 820 | loss 0.5932
step 830 | loss 0.4381
step 840 | loss 0.3831
step 850 | loss 0.5349
step 860 | loss 0.5219
step 870 | loss 0.5187
step 880 | loss 0.5603
step 890 | loss 0.3474
step 900 | loss 0.3356
step 910 | loss 0.5301
step 920 | loss 0.4241
step 930 | loss 0.3919
step 940 | loss 0.3592
step 950 | loss 0.3558
step 960 | loss 0.5536
step 970 | loss 0.5404
step 980 | loss 0.4317
step 990 | loss 0.4012
step 1000: train loss 0.4417, val loss 1.2013
step 1000 | loss 0.8164
step 1010 | loss 0.4030
step 1020 | loss 0.4941
step 1030 | loss 0.3304
step 1040 | loss 0.5359
step 1050 | loss 0.3482
step 1060 | loss 0.3628
step 1070 | loss 0.3390
step 1080 | loss 0.4059
step 1090 | loss 0.4155
step 1100 | loss 0.4360
step 1110 | loss 0.3914
step 1120 | loss 0.3731
step 1130 | loss 0.5274
step 1140 | loss 0.3224
step 1150 | loss 0.4097
step 1160 | loss 0.3821
step 1170 | loss 0.2753
step 1180 | loss 0.4989
step 1190 | loss 0.3921
step 1200 | loss 0.1487
step 1210 | loss 0.4025
step 1220 | loss 0.3412
step 1230 | loss 0.3750
step 1240 | loss 0.2550
step 1250: train loss 0.3969, val loss 1.1108
step 1250 | loss 0.3492
step 1260 | loss 0.5466
step 1270 | loss 0.4484
step 1280 | loss 0.3823
step 1290 | loss 0.4782
step 1300 | loss 0.3000
step 1310 | loss 0.3602
step 1320 | loss 0.2634
step 1330 | loss 0.5059
step 1340 | loss 0.3601
step 1350 | loss 0.4180
step 1360 | loss 0.3476
step 1370 | loss 0.4399
step 1380 | loss 0.4312
step 1390 | loss 0.4347
step 1400 | loss 0.5213
step 1410 | loss 0.4636
step 1420 | loss 0.3579
step 1430 | loss 0.4577
step 1440 | loss 0.4515
step 1450 | loss 0.3341
step 1460 | loss 0.3362
step 1470 | loss 0.3268
step 1480 | loss 0.4540
step 1490 | loss 0.3759
step 1500: train loss 0.3690, val loss 1.0335
step 1500 | loss 0.3027
step 1510 | loss 0.4082
step 1520 | loss 0.2482
step 1530 | loss 0.4049
step 1540 | loss 0.3083
step 1550 | loss 0.5037
step 1560 | loss 0.3726
step 1570 | loss 0.2599
step 1580 | loss 0.3311
step 1590 | loss 0.3365
step 1600 | loss 0.3373
step 1610 | loss 0.4674
step 1620 | loss 0.3996
step 1630 | loss 0.1974
step 1640 | loss 0.3262
step 1650 | loss 0.2858
step 1660 | loss 0.5289
step 1670 | loss 0.4915
step 1680 | loss 0.3756
step 1690 | loss 0.1961
step 1700 | loss 0.3658
step 1710 | loss 0.3240
step 1720 | loss 0.1790
step 1730 | loss 0.2932
step 1740 | loss 0.3620
step 1750: train loss 0.3463, val loss 0.9704
step 1750 | loss 0.4203
step 1760 | loss 0.2995
step 1770 | loss 0.3062
step 1780 | loss 0.3652
step 1790 | loss 0.2942
step 1800 | loss 0.3455
step 1810 | loss 0.3690
step 1820 | loss 0.4131
step 1830 | loss 0.3664
step 1840 | loss 0.4734
step 1850 | loss 0.2611
step 1860 | loss 0.3988
step 1870 | loss 0.1738
step 1880 | loss 0.2562
step 1890 | loss 0.3105
step 1900 | loss 0.3989
step 1910 | loss 0.1558
step 1920 | loss 0.3683
step 1930 | loss 0.2592
step 1940 | loss 0.2978
step 1950 | loss 0.3630
step 1960 | loss 0.3645
step 1970 | loss 0.3670
step 1980 | loss 0.4063
step 1990 | loss 0.2426
step 2000: train loss 0.3203, val loss 0.9488
step 2000 | loss 0.2397
step 2010 | loss 0.3831
step 2020 | loss 0.3292
step 2030 | loss 0.3704
step 2040 | loss 0.3901
step 2050 | loss 0.2835
step 2060 | loss 0.1952
step 2070 | loss 0.3156
step 2080 | loss 0.3498
step 2090 | loss 0.3604
step 2100 | loss 0.3409
step 2110 | loss 0.2655
step 2120 | loss 0.3769
step 2130 | loss 0.1849
step 2140 | loss 0.1855
step 2150 | loss 0.3967
step 2160 | loss 0.2835
step 2170 | loss 0.1274
step 2180 | loss 0.2658
step 2190 | loss 0.3370
step 2200 | loss 0.2283
step 2210 | loss 0.5277
step 2220 | loss 0.2757
step 2230 | loss 0.2259
step 2240 | loss 0.2227
step 2250: train loss 0.3066, val loss 0.8907
step 2250 | loss 0.3842
step 2260 | loss 0.3508
step 2270 | loss 0.4332
step 2280 | loss 0.4496
step 2290 | loss 0.2488
step 2300 | loss 0.2411
step 2310 | loss 0.3035
step 2320 | loss 0.3384
step 2330 | loss 0.2268
step 2340 | loss 0.2620
step 2350 | loss 0.3030
step 2360 | loss 0.1997
step 2370 | loss 0.2551
step 2380 | loss 0.3443
step 2390 | loss 0.4190
step 2400 | loss 0.2968
step 2410 | loss 0.3932
step 2420 | loss 0.3544
step 2430 | loss 0.2491
step 2440 | loss 0.3163
step 2450 | loss 0.3443
step 2460 | loss 0.2976
step 2470 | loss 0.1597
step 2480 | loss 0.4071
step 2490 | loss 0.3224
step 2500: train loss 0.3074, val loss 0.8760
step 2500 | loss 0.2580
step 2510 | loss 0.2199
step 2520 | loss 0.1826
step 2530 | loss 0.2147
step 2540 | loss 0.2558
step 2550 | loss 0.2798
step 2560 | loss 0.2811
step 2570 | loss 0.2847
step 2580 | loss 0.2522
step 2590 | loss 0.2478
step 2600 | loss 0.1839
step 2610 | loss 0.3290
step 2620 | loss 0.2547
step 2630 | loss 0.2226
step 2640 | loss 0.2161
step 2650 | loss 0.3874
step 2660 | loss 0.3293
step 2670 | loss 0.2439
step 2680 | loss 0.2535
step 2690 | loss 0.1339
step 2700 | loss 0.2294
step 2710 | loss 0.2655
step 2720 | loss 0.2007
step 2730 | loss 0.2663
step 2740 | loss 0.2880
step 2750: train loss 0.3006, val loss 0.8681
step 2750 | loss 0.3729
step 2760 | loss 0.2592
step 2770 | loss 0.3167
step 2780 | loss 0.2023
step 2790 | loss 0.2796
step 2800 | loss 0.3334
step 2810 | loss 0.1943
step 2820 | loss 0.2635
step 2830 | loss 0.2538
step 2840 | loss 0.2870
step 2850 | loss 0.2675
step 2860 | loss 0.2876
step 2870 | loss 0.2586
step 2880 | loss 0.2798
step 2890 | loss 0.2301
step 2900 | loss 0.2363
step 2910 | loss 0.3076
step 2920 | loss 0.1670
step 2930 | loss 0.2540
step 2940 | loss 0.2073
step 2950 | loss 0.1918
step 2960 | loss 0.3663
step 2970 | loss 0.2759
step 2980 | loss 0.2751
step 2990 | loss 0.1977
Running final evaluation...
step 3000: train loss 0.2864, val loss 0.8320
Done.
[2/4] Training RNN SMALL...
Model Parameters: 6.41M
Training RNN rnn_small...
Eff. Batch: 256 (Physical 32 * Accum 8)
step 0: train loss 4.5930, val loss 4.5920
step 0 | loss 4.5932
step 10 | loss 3.1119
step 20 | loss 3.1012
step 30 | loss 3.1122
step 40 | loss 3.0767
step 50 | loss 2.9581
step 60 | loss 3.1751
step 70 | loss 3.1502
step 80 | loss 2.9830
step 90 | loss 2.9733
step 100 | loss 2.9653
step 110 | loss 3.0397
step 120 | loss 3.0294
step 130 | loss 2.7754
step 140 | loss 2.3694
step 150 | loss 2.4047
step 160 | loss 2.1318
step 170 | loss 2.0135
step 180 | loss 2.0910
step 190 | loss 1.3799
step 200 | loss 1.5501
step 210 | loss 1.3327
step 220 | loss 1.5393
step 230 | loss 1.3580
step 240 | loss 1.5875
step 250: train loss 1.3487, val loss 2.4595
step 250 | loss 1.4754
step 260 | loss 1.2811
step 270 | loss 1.2690
step 280 | loss 1.0448
step 290 | loss 1.1493
step 300 | loss 1.4363
step 310 | loss 1.0452
step 320 | loss 0.9767
step 330 | loss 0.6996
step 340 | loss 1.1323
step 350 | loss 0.9584
step 360 | loss 1.0389
step 370 | loss 0.8823
step 380 | loss 0.7816
step 390 | loss 0.9869
step 400 | loss 0.9969
step 410 | loss 0.5535
step 420 | loss 0.9702
step 430 | loss 1.2674
step 440 | loss 0.7371
step 450 | loss 0.7047
step 460 | loss 0.6369
step 470 | loss 0.6784
step 480 | loss 0.7223
step 490 | loss 0.5854
step 500: train loss 0.7038, val loss 1.6291
step 500 | loss 0.5549
step 510 | loss 0.5004
step 520 | loss 0.5915
step 530 | loss 0.4791
step 540 | loss 0.5121
step 550 | loss 0.6967
step 560 | loss 0.6249
step 570 | loss 0.7770
step 580 | loss 0.6603
step 590 | loss 0.4927
step 600 | loss 0.5496
step 610 | loss 0.7657
step 620 | loss 0.6075
step 630 | loss 0.6898
step 640 | loss 0.4250
step 650 | loss 0.6799
step 660 | loss 0.3981
step 670 | loss 0.6274
step 680 | loss 0.5313
step 690 | loss 0.4292
step 700 | loss 0.4618
step 710 | loss 0.4297
step 720 | loss 0.5903
step 730 | loss 0.5430
step 740 | loss 0.6471
step 750: train loss 0.5288, val loss 1.3165
step 750 | loss 0.3606
step 760 | loss 0.3433
step 770 | loss 0.7010
step 780 | loss 0.4600
step 790 | loss 0.4081
step 800 | loss 0.5008
step 810 | loss 0.4779
step 820 | loss 0.5620
step 830 | loss 0.6362
step 840 | loss 0.5509
step 850 | loss 0.4652
step 860 | loss 0.7476
step 870 | loss 0.3625
step 880 | loss 0.5836
step 890 | loss 0.4987
step 900 | loss 0.4232
step 910 | loss 0.4313
step 920 | loss 0.4393
step 930 | loss 0.5266
step 940 | loss 0.5314
step 950 | loss 0.4059
step 960 | loss 0.3867
step 970 | loss 0.4588
step 980 | loss 0.3628
step 990 | loss 0.6283
step 1000: train loss 0.4778, val loss 1.1829
step 1000 | loss 0.4755
step 1010 | loss 0.5028
step 1020 | loss 0.4691
step 1030 | loss 0.3889
step 1040 | loss 0.5169
step 1050 | loss 0.5935
step 1060 | loss 0.3941
step 1070 | loss 0.3383
step 1080 | loss 0.3137
step 1090 | loss 0.4621
step 1100 | loss 0.4635
step 1110 | loss 0.3965
step 1120 | loss 0.4897
step 1130 | loss 0.4133
step 1140 | loss 0.4659
step 1150 | loss 0.3246
step 1160 | loss 0.3722
step 1170 | loss 0.4144
step 1180 | loss 0.5101
step 1190 | loss 0.5098
step 1200 | loss 0.5647
step 1210 | loss 0.4038
step 1220 | loss 0.2979
step 1230 | loss 0.5509
step 1240 | loss 0.3539
step 1250: train loss 0.4231, val loss 1.1112
step 1250 | loss 0.3588
step 1260 | loss 0.4605
step 1270 | loss 0.4705
step 1280 | loss 0.4687
step 1290 | loss 0.2877
step 1300 | loss 0.5165
step 1310 | loss 0.3101
step 1320 | loss 0.4112
step 1330 | loss 0.4832
step 1340 | loss 0.5586
step 1350 | loss 0.1405
step 1360 | loss 0.3560
step 1370 | loss 0.5671
step 1380 | loss 0.4151
step 1390 | loss 0.3865
step 1400 | loss 0.4005
step 1410 | loss 0.2347
step 1420 | loss 0.4105
step 1430 | loss 0.2175
step 1440 | loss 0.3757
step 1450 | loss 0.4633
step 1460 | loss 0.2158
step 1470 | loss 0.3047
step 1480 | loss 0.2355
step 1490 | loss 0.4827
step 1500: train loss 0.3809, val loss 1.0470
step 1500 | loss 0.4303
step 1510 | loss 0.2547
step 1520 | loss 0.3085
step 1530 | loss 0.3632
step 1540 | loss 0.3635
step 1550 | loss 0.3018
step 1560 | loss 0.2524
step 1570 | loss 0.3307
step 1580 | loss 0.3666
step 1590 | loss 0.2399
step 1600 | loss 0.2324
step 1610 | loss 0.3914
step 1620 | loss 0.2265
step 1630 | loss 0.3537
step 1640 | loss 0.3287
step 1650 | loss 0.3480
step 1660 | loss 0.3542
step 1670 | loss 0.3393
step 1680 | loss 0.3895
step 1690 | loss 0.3288
step 1700 | loss 0.3778
step 1710 | loss 0.4177
step 1720 | loss 0.3856
step 1730 | loss 0.4783
step 1740 | loss 0.3588
step 1750: train loss 0.3618, val loss 1.0009
step 1750 | loss 0.3090
step 1760 | loss 0.4789
step 1770 | loss 0.5348
step 1780 | loss 0.2289
step 1790 | loss 0.1698
step 1800 | loss 0.3883
step 1810 | loss 0.2685
step 1820 | loss 0.1891
step 1830 | loss 0.3450
step 1840 | loss 0.2000
step 1850 | loss 0.4850
step 1860 | loss 0.3930
step 1870 | loss 0.3341
step 1880 | loss 0.3101
step 1890 | loss 0.4670
step 1900 | loss 0.3800
step 1910 | loss 0.3462
step 1920 | loss 0.2843
step 1930 | loss 0.3839
step 1940 | loss 0.3431
step 1950 | loss 0.3808
step 1960 | loss 0.3246
step 1970 | loss 0.2963
step 1980 | loss 0.2755
step 1990 | loss 0.2972
step 2000: train loss 0.3316, val loss 0.9575
step 2000 | loss 0.2828
step 2010 | loss 0.3454
step 2020 | loss 0.3003
step 2030 | loss 0.2038
step 2040 | loss 0.4897
step 2050 | loss 0.4210
step 2060 | loss 0.4259
step 2070 | loss 0.3893
step 2080 | loss 0.2943
step 2090 | loss 0.3642
step 2100 | loss 0.3652
step 2110 | loss 0.3422
step 2120 | loss 0.3091
step 2130 | loss 0.3073
step 2140 | loss 0.2442
step 2150 | loss 0.3154
step 2160 | loss 0.4101
step 2170 | loss 0.3747
step 2180 | loss 0.4300
step 2190 | loss 0.2663
step 2200 | loss 0.2150
step 2210 | loss 0.2758
step 2220 | loss 0.3311
step 2230 | loss 0.3221
step 2240 | loss 0.3934
step 2250: train loss 0.3143, val loss 0.9003
step 2250 | loss 0.4137
step 2260 | loss 0.1860
step 2270 | loss 0.3830
step 2280 | loss 0.4096
step 2290 | loss 0.3041
step 2300 | loss 0.3802
step 2310 | loss 0.2434
step 2320 | loss 0.4319
step 2330 | loss 0.4036
step 2340 | loss 0.1792
step 2350 | loss 0.3748
step 2360 | loss 0.2773
step 2370 | loss 0.2395
step 2380 | loss 0.5520
step 2390 | loss 0.4527
step 2400 | loss 0.1718
step 2410 | loss 0.2770
step 2420 | loss 0.3535
step 2430 | loss 0.2821
step 2440 | loss 0.3112
step 2450 | loss 0.1848
step 2460 | loss 0.3766
step 2470 | loss 0.4055
step 2480 | loss 0.2953
step 2490 | loss 0.2298
step 2500: train loss 0.2977, val loss 0.8852
step 2500 | loss 0.2070
step 2510 | loss 0.5606
step 2520 | loss 0.1798
step 2530 | loss 0.5092
step 2540 | loss 0.3236
step 2550 | loss 0.2785
step 2560 | loss 0.2924
step 2570 | loss 0.2937
step 2580 | loss 0.2472
step 2590 | loss 0.2883
step 2600 | loss 0.3588
step 2610 | loss 0.3596
step 2620 | loss 0.3869
step 2630 | loss 0.3088
step 2640 | loss 0.2996
step 2650 | loss 0.3327
step 2660 | loss 0.4010
step 2670 | loss 0.2922
step 2680 | loss 0.3649
step 2690 | loss 0.2592
step 2700 | loss 0.3642
step 2710 | loss 0.2169
step 2720 | loss 0.2654
step 2730 | loss 0.3601
step 2740 | loss 0.2927
step 2750: train loss 0.2866, val loss 0.8659
step 2750 | loss 0.1809
step 2760 | loss 0.2198
step 2770 | loss 0.3131
step 2780 | loss 0.1561
step 2790 | loss 0.3529
step 2800 | loss 0.1842
step 2810 | loss 0.3373
step 2820 | loss 0.2255
step 2830 | loss 0.3285
step 2840 | loss 0.2906
step 2850 | loss 0.2464
step 2860 | loss 0.2129
step 2870 | loss 0.2736
step 2880 | loss 0.3000
step 2890 | loss 0.2271
step 2900 | loss 0.3234
step 2910 | loss 0.2305
step 2920 | loss 0.3338
step 2930 | loss 0.3089
step 2940 | loss 0.1836
step 2950 | loss 0.2506
step 2960 | loss 0.2232
step 2970 | loss 0.3374
step 2980 | loss 0.3991
step 2990 | loss 0.3544
Running final evaluation...
step 3000: train loss 0.2832, val loss 0.8393
Done.
[3/4] Training RNN MEDIUM...
Model Parameters: 25.90M
Training RNN rnn_medium...
Eff. Batch: 256 (Physical 32 * Accum 8)
step 0: train loss 4.5938, val loss 4.5924
step 0 | loss 4.5924
step 10 | loss 3.1270
step 20 | loss 3.1595
step 30 | loss 3.0200
step 40 | loss 3.0601
step 50 | loss 3.3382
step 60 | loss 3.0294
step 70 | loss 3.0219
step 80 | loss 3.2597
step 90 | loss 3.0732
step 100 | loss 3.0769
step 110 | loss 3.0635
step 120 | loss 3.0224
step 130 | loss 2.9577
step 140 | loss 2.9702
step 150 | loss 2.9098
step 160 | loss 2.4946
step 170 | loss 2.4391
step 180 | loss 2.1939
step 190 | loss 1.7282
step 200 | loss 1.4285
step 210 | loss 1.5677
step 220 | loss 1.4975
step 230 | loss 1.3511
step 240 | loss 1.0857
step 250: train loss 1.2045, val loss 2.2618
step 250 | loss 0.9543
step 260 | loss 1.3079
step 270 | loss 1.2364
step 280 | loss 1.1172
step 290 | loss 1.0987
step 300 | loss 1.1459
step 310 | loss 1.0437
step 320 | loss 0.8978
step 330 | loss 0.9389
step 340 | loss 0.7246
step 350 | loss 1.0910
step 360 | loss 1.0912
step 370 | loss 0.8412
step 380 | loss 0.7445
step 390 | loss 1.0361
step 400 | loss 0.8518
step 410 | loss 0.6564
step 420 | loss 0.6301
step 430 | loss 0.7605
step 440 | loss 0.7275
step 450 | loss 0.7501
step 460 | loss 0.6152
step 470 | loss 0.8453
step 480 | loss 0.6282
step 490 | loss 0.4865
step 500: train loss 0.6783, val loss 1.6329
step 500 | loss 0.9233
step 510 | loss 0.6444
step 520 | loss 0.7729
step 530 | loss 0.6668
step 540 | loss 0.5670
step 550 | loss 0.4379
step 560 | loss 0.8533
step 570 | loss 0.6629
step 580 | loss 0.5157
step 590 | loss 0.4725
step 600 | loss 0.6787
step 610 | loss 0.5808
step 620 | loss 0.6615
step 630 | loss 0.5001
step 640 | loss 0.6654
step 650 | loss 0.7173
step 660 | loss 0.8680
step 670 | loss 0.5489
step 680 | loss 0.5750
step 690 | loss 0.3679
step 700 | loss 0.4819
step 710 | loss 0.4991
step 720 | loss 0.5900
step 730 | loss 0.5283
step 740 | loss 0.3962
step 750: train loss 0.5103, val loss 1.3229
step 750 | loss 0.4233
step 760 | loss 0.7264
step 770 | loss 0.5080
step 780 | loss 0.5973
step 790 | loss 0.6822
step 800 | loss 0.4150
step 810 | loss 0.4135
step 820 | loss 0.4137
step 830 | loss 0.3491
step 840 | loss 0.3066
step 850 | loss 0.6718
step 860 | loss 0.4824
step 870 | loss 0.4385
step 880 | loss 0.5022
step 890 | loss 0.7193
step 900 | loss 0.4005
step 910 | loss 0.5659
step 920 | loss 0.4857
step 930 | loss 0.7743
step 940 | loss 0.3467
step 950 | loss 0.4998
step 960 | loss 0.4004
step 970 | loss 0.3425
step 980 | loss 0.3926
step 990 | loss 0.5408
step 1000: train loss 0.4433, val loss 1.2406
step 1000 | loss 0.5291
step 1010 | loss 0.5281
step 1020 | loss 0.2672
step 1030 | loss 0.3826
step 1040 | loss 0.3117
step 1050 | loss 0.3657
step 1060 | loss 0.4181
step 1070 | loss 0.5716
step 1080 | loss 0.3462
step 1090 | loss 0.6196
step 1100 | loss 0.5095
step 1110 | loss 0.5663
step 1120 | loss 0.3338
step 1130 | loss 0.4622
step 1140 | loss 0.3643
step 1150 | loss 0.4408
step 1160 | loss 0.3417
step 1170 | loss 0.4540
step 1180 | loss 0.4999
step 1190 | loss 0.3167
step 1200 | loss 0.4727
step 1210 | loss 0.2937
step 1220 | loss 0.3061
step 1230 | loss 0.5736
step 1240 | loss 0.3384
step 1250: train loss 0.3951, val loss 1.1532
step 1250 | loss 0.5306
step 1260 | loss 0.4621
step 1270 | loss 0.5833
step 1280 | loss 0.3469
step 1290 | loss 0.3603
step 1300 | loss 0.3906
step 1310 | loss 0.4950
step 1320 | loss 0.3791
step 1330 | loss 0.4297
step 1340 | loss 0.3769
step 1350 | loss 0.5060
step 1360 | loss 0.4267
step 1370 | loss 0.3788
step 1380 | loss 0.3509
step 1390 | loss 0.3617
step 1400 | loss 0.3448
step 1410 | loss 0.4224
step 1420 | loss 0.5198
step 1430 | loss 0.2634
step 1440 | loss 0.3354
step 1450 | loss 0.4356
step 1460 | loss 0.4030
step 1470 | loss 0.2167
step 1480 | loss 0.4423
step 1490 | loss 0.4157
step 1500: train loss 0.3960, val loss 1.0897
step 1500 | loss 0.4190
step 1510 | loss 0.3247
step 1520 | loss 0.4521
step 1530 | loss 0.4412
step 1540 | loss 0.3553
step 1550 | loss 0.2499
step 1560 | loss 0.4751
step 1570 | loss 0.4477
step 1580 | loss 0.3034
step 1590 | loss 0.3080
step 1600 | loss 0.4234
step 1610 | loss 0.2184
step 1620 | loss 0.4833
step 1630 | loss 0.4811
step 1640 | loss 0.3649
step 1650 | loss 0.3344
step 1660 | loss 0.3416
step 1670 | loss 0.3842
step 1680 | loss 0.5595
step 1690 | loss 0.2751
step 1700 | loss 0.3543
step 1710 | loss 0.4349
step 1720 | loss 0.4449
step 1730 | loss 0.3958
step 1740 | loss 0.4164
step 1750: train loss 0.3573, val loss 1.0468
step 1750 | loss 0.3499
step 1760 | loss 0.2613
step 1770 | loss 0.3370
step 1780 | loss 0.4421
step 1790 | loss 0.2999
step 1800 | loss 0.2332
step 1810 | loss 0.2800
step 1820 | loss 0.3783
step 1830 | loss 0.3878
step 1840 | loss 0.3143
step 1850 | loss 0.3683
step 1860 | loss 0.3018
step 1870 | loss 0.2474
step 1880 | loss 0.3542
step 1890 | loss 0.5686
step 1900 | loss 0.4482
step 1910 | loss 0.2904
step 1920 | loss 0.3143
step 1930 | loss 0.3548
step 1940 | loss 0.2590
step 1950 | loss 0.3436
step 1960 | loss 0.3777
step 1970 | loss 0.3616
step 1980 | loss 0.2778
step 1990 | loss 0.2788
step 2000: train loss 0.3301, val loss 0.9920
step 2000 | loss 0.3378
step 2010 | loss 0.4709
step 2020 | loss 0.2830
step 2030 | loss 0.2387
step 2040 | loss 0.3225
step 2050 | loss 0.4889
step 2060 | loss 0.3353
step 2070 | loss 0.3604
step 2080 | loss 0.4649
step 2090 | loss 0.3098
step 2100 | loss 0.4577
step 2110 | loss 0.5341
step 2120 | loss 0.2587
step 2130 | loss 0.2795
step 2140 | loss 0.4464
step 2150 | loss 0.5098
step 2160 | loss 0.4321
step 2170 | loss 0.3082
step 2180 | loss 0.4063
step 2190 | loss 0.2602
step 2200 | loss 0.3247
step 2210 | loss 0.4177
step 2220 | loss 0.2570
step 2230 | loss 0.3450
step 2240 | loss 0.3094
step 2250: train loss 0.3258, val loss 0.9675
step 2250 | loss 0.3106
step 2260 | loss 0.3700
step 2270 | loss 0.2870
step 2280 | loss 0.3418
step 2290 | loss 0.4820
step 2300 | loss 0.3611
step 2310 | loss 0.5738
step 2320 | loss 0.3963
step 2330 | loss 0.2344
step 2340 | loss 0.2857
step 2350 | loss 0.4340
step 2360 | loss 0.3393
step 2370 | loss 0.2267
step 2380 | loss 0.2738
step 2390 | loss 0.2388
step 2400 | loss 0.3714
step 2410 | loss 0.2460
step 2420 | loss 0.3153
step 2430 | loss 0.2750
step 2440 | loss 0.2568
step 2450 | loss 0.3534
step 2460 | loss 0.2851
step 2470 | loss 0.2654
step 2480 | loss 0.1333
step 2490 | loss 0.3595
step 2500: train loss 0.3263, val loss 0.9482
step 2500 | loss 0.2975
step 2510 | loss 0.2470
step 2520 | loss 0.3283
step 2530 | loss 0.3053
step 2540 | loss 0.3779
step 2550 | loss 0.3737
step 2560 | loss 0.3360
step 2570 | loss 0.3596
step 2580 | loss 0.2489
step 2590 | loss 0.2230
step 2600 | loss 0.2755
step 2610 | loss 0.3021
step 2620 | loss 0.2216
step 2630 | loss 0.2454
step 2640 | loss 0.3176
step 2650 | loss 0.4231
step 2660 | loss 0.2334
step 2670 | loss 0.3045
step 2680 | loss 0.3089
step 2690 | loss 0.5116
step 2700 | loss 0.4396
step 2710 | loss 0.3876
step 2720 | loss 0.3322
step 2730 | loss 0.2371
step 2740 | loss 0.3012
step 2750: train loss 0.2974, val loss 0.9034
step 2750 | loss 0.3282
step 2760 | loss 0.2070
step 2770 | loss 0.3331
step 2780 | loss 0.2651
step 2790 | loss 0.3247
step 2800 | loss 0.2021
step 2810 | loss 0.3886
step 2820 | loss 0.2921
step 2830 | loss 0.2712
step 2840 | loss 0.2399
step 2850 | loss 0.2354
step 2860 | loss 0.2635
step 2870 | loss 0.1871
step 2880 | loss 0.2519
step 2890 | loss 0.2250
step 2900 | loss 0.4094
step 2910 | loss 0.3403
step 2920 | loss 0.3354
step 2930 | loss 0.3680
step 2940 | loss 0.2285
step 2950 | loss 0.4196
step 2960 | loss 0.2034
step 2970 | loss 0.2713
step 2980 | loss 0.3486
step 2990 | loss 0.2992
Running final evaluation...
step 3000: train loss 0.2873, val loss 0.8786
Done.
[4/4] Training RNN LARGE...
Model Parameters: 87.02M
Training RNN rnn_large...
Eff. Batch: 256 (Physical 32 * Accum 8)
step 0: train loss 4.5966, val loss 4.5964
step 0 | loss 4.5964
step 10 | loss 3.0712
step 20 | loss 3.0832
step 30 | loss 3.0607
step 40 | loss 2.9955
step 50 | loss 3.0981
step 60 | loss 3.0716
step 70 | loss 3.1779
step 80 | loss 3.0542
step 90 | loss 2.9958
step 100 | loss 3.1455
step 110 | loss 3.0374
step 120 | loss 2.8566
step 130 | loss 2.0913
step 140 | loss 1.6387
step 150 | loss 1.9809
step 160 | loss 1.7699
step 170 | loss 1.3507
step 180 | loss 1.2671
step 190 | loss 0.9235
step 200 | loss 1.0882
step 210 | loss 1.1646
step 220 | loss 1.0208
step 230 | loss 1.1499
step 240 | loss 0.9089
step 250: train loss 0.8407, val loss 1.8253
step 250 | loss 1.0176
step 260 | loss 0.8100
step 270 | loss 0.6951
step 280 | loss 0.7148
step 290 | loss 0.5627
step 300 | loss 0.7273
step 310 | loss 0.6789
step 320 | loss 0.8002
step 330 | loss 0.9891
step 340 | loss 0.4852
step 350 | loss 0.6214
step 360 | loss 0.8383
step 370 | loss 0.6017
step 380 | loss 0.5973
step 390 | loss 0.4352
step 400 | loss 0.6959
step 410 | loss 0.3643
step 420 | loss 0.7098
step 430 | loss 0.4862
step 440 | loss 0.5476
step 450 | loss 0.4667
step 460 | loss 0.3597
step 470 | loss 0.4854
step 480 | loss 0.5570
step 490 | loss 0.4101
step 500: train loss 0.5112, val loss 1.3294
step 500 | loss 0.6098
step 510 | loss 0.7737
step 520 | loss 0.6252
step 530 | loss 0.6360
step 540 | loss 0.5110
step 550 | loss 0.4686
step 560 | loss 0.5377
step 570 | loss 0.5205
step 580 | loss 0.4663
step 590 | loss 0.5414
step 600 | loss 0.5206
step 610 | loss 0.3655
step 620 | loss 0.2884
step 630 | loss 0.4832
step 640 | loss 0.4138
step 650 | loss 0.5106
step 660 | loss 0.2560
step 670 | loss 0.5312
step 680 | loss 0.4637
step 690 | loss 0.5286
step 700 | loss 0.4681
step 710 | loss 0.3786
step 720 | loss 0.3582
step 730 | loss 0.5307
step 740 | loss 0.2878
step 750: train loss 0.4427, val loss 1.2170
step 750 | loss 0.2696
step 760 | loss 0.5569
step 770 | loss 0.4694
step 780 | loss 0.3456
step 790 | loss 0.3080
step 800 | loss 0.5285
step 810 | loss 0.2999
step 820 | loss 0.4028
step 830 | loss 0.2005
step 840 | loss 0.4438
step 850 | loss 0.4144
step 860 | loss 0.3093
step 870 | loss 0.4550
step 880 | loss 0.3529
step 890 | loss 0.3603
step 900 | loss 0.4236
step 910 | loss 0.2454
step 920 | loss 0.3789
step 930 | loss 0.3652
step 940 | loss 0.3035
step 950 | loss 0.4161
step 960 | loss 0.4923
step 970 | loss 0.2238
step 980 | loss 0.3623
step 990 | loss 0.3593
step 1000: train loss 0.4107, val loss 1.1206
step 1000 | loss 0.4330
step 1010 | loss 0.3242
step 1020 | loss 0.5067
step 1030 | loss 0.4513
step 1040 | loss 0.3560
step 1050 | loss 0.3150
step 1060 | loss 0.4168
step 1070 | loss 0.4301
step 1080 | loss 0.5111
step 1090 | loss 0.3752
step 1100 | loss 0.4006
step 1110 | loss 0.4995
step 1120 | loss 0.3322
step 1130 | loss 0.3527
step 1140 | loss 0.5628
step 1150 | loss 0.4028
step 1160 | loss 0.4811
step 1170 | loss 0.4545
step 1180 | loss 0.3811
step 1190 | loss 0.2544
step 1200 | loss 0.5166
step 1210 | loss 0.4236
step 1220 | loss 0.3114
step 1230 | loss 0.3960
step 1240 | loss 0.2648
step 1250: train loss 0.3640, val loss 1.0570
step 1250 | loss 0.5691
step 1260 | loss 0.3077
step 1270 | loss 0.3597
step 1280 | loss 0.4775
step 1290 | loss 0.3074
step 1300 | loss 0.3144
step 1310 | loss 0.3146
step 1320 | loss 0.2225
step 1330 | loss 0.3148
step 1340 | loss 0.2958
step 1350 | loss 0.4372
step 1360 | loss 0.4692
step 1370 | loss 0.2452
step 1380 | loss 0.3178
step 1390 | loss 0.3734
step 1400 | loss 0.5334
step 1410 | loss 0.3087
step 1420 | loss 0.3406
step 1430 | loss 0.2914
step 1440 | loss 0.3415
step 1450 | loss 0.3473
step 1460 | loss 0.3030
step 1470 | loss 0.3759
step 1480 | loss 0.3251
step 1490 | loss 0.3603
step 1500: train loss 0.3353, val loss 1.0092
step 1500 | loss 0.3883
step 1510 | loss 0.4032
step 1520 | loss 0.2289
step 1530 | loss 0.4060
step 1540 | loss 0.3139
step 1550 | loss 0.3081
step 1560 | loss 0.4110
step 1570 | loss 0.3849
step 1580 | loss 0.3881
step 1590 | loss 0.2201
step 1600 | loss 0.2490
step 1610 | loss 0.1747
step 1620 | loss 0.2494
step 1630 | loss 0.3355
step 1640 | loss 0.3245
step 1650 | loss 0.4665
step 1660 | loss 0.3845
step 1670 | loss 0.2612
step 1680 | loss 0.3439
step 1690 | loss 0.2839
step 1700 | loss 0.3861
step 1710 | loss 0.4355
step 1720 | loss 0.2370
step 1730 | loss 0.4279
step 1740 | loss 0.2535
step 1750: train loss 0.3266, val loss 0.9648
step 1750 | loss 0.2817
step 1760 | loss 0.3580
step 1770 | loss 0.3846
step 1780 | loss 0.3212
step 1790 | loss 0.4721
step 1800 | loss 0.4850
step 1810 | loss 0.4181
step 1820 | loss 0.2020
step 1830 | loss 0.3440
step 1840 | loss 0.3439
step 1850 | loss 0.4312
step 1860 | loss 0.3946
step 1870 | loss 0.2352
step 1880 | loss 0.2565
step 1890 | loss 0.3564
step 1900 | loss 0.2167
step 1910 | loss 0.2625
step 1920 | loss 0.4052
step 1930 | loss 0.2877
step 1940 | loss 0.2320
step 1950 | loss 0.3827
step 1960 | loss 0.2157
step 1970 | loss 0.5008
step 1980 | loss 0.3537
step 1990 | loss 0.4277
step 2000: train loss 0.3147, val loss 0.9357
step 2000 | loss 0.4190
step 2010 | loss 0.3356
step 2020 | loss 0.4453
step 2030 | loss 0.4242
step 2040 | loss 0.2358
step 2050 | loss 0.2128
step 2060 | loss 0.2981
step 2070 | loss 0.3161
step 2080 | loss 0.3458
step 2090 | loss 0.2135
step 2100 | loss 0.3695
step 2110 | loss 0.2984
step 2120 | loss 0.1987
step 2130 | loss 0.2976
step 2140 | loss 0.2749
step 2150 | loss 0.3690
step 2160 | loss 0.2715
step 2170 | loss 0.4081
step 2180 | loss 0.3241
step 2190 | loss 0.2342
step 2200 | loss 0.3479
step 2210 | loss 0.3083
step 2220 | loss 0.4141
step 2230 | loss 0.2795
step 2240 | loss 0.2817
step 2250: train loss 0.3125, val loss 0.9128
step 2250 | loss 0.1918
step 2260 | loss 0.3323
step 2270 | loss 0.5003
step 2280 | loss 0.3538
step 2290 | loss 0.3005
step 2300 | loss 0.3014
step 2310 | loss 0.3150
step 2320 | loss 0.2928
step 2330 | loss 0.3524
step 2340 | loss 0.3084
step 2350 | loss 0.3249
step 2360 | loss 0.3178
step 2370 | loss 0.2686
step 2380 | loss 0.2489
step 2390 | loss 0.2887
step 2400 | loss 0.2424
step 2410 | loss 0.2834
step 2420 | loss 0.2659
step 2430 | loss 0.2764
step 2440 | loss 0.3266
step 2450 | loss 0.3084
step 2460 | loss 0.2560
step 2470 | loss 0.2047
step 2480 | loss 0.3403
step 2490 | loss 0.2329
step 2500: train loss 0.2978, val loss 0.8821
step 2500 | loss 0.3017
step 2510 | loss 0.3587
step 2520 | loss 0.3042
step 2530 | loss 0.2512
step 2540 | loss 0.2541
step 2550 | loss 0.3544
step 2560 | loss 0.3584
step 2570 | loss 0.2678
step 2580 | loss 0.2873
step 2590 | loss 0.4658
step 2600 | loss 0.1601
step 2610 | loss 0.2268
step 2620 | loss 0.4168
step 2630 | loss 0.1846
step 2640 | loss 0.2050
step 2650 | loss 0.3137
step 2660 | loss 0.2151
step 2670 | loss 0.2642
step 2680 | loss 0.3284
step 2690 | loss 0.1945
step 2700 | loss 0.3546
step 2710 | loss 0.2358
step 2720 | loss 0.3191
step 2730 | loss 0.3078
step 2740 | loss 0.1430
step 2750: train loss 0.2933, val loss 0.8623
step 2750 | loss 0.2942
step 2760 | loss 0.3854
step 2770 | loss 0.3694
step 2780 | loss 0.3007
step 2790 | loss 0.3257
step 2800 | loss 0.4169
step 2810 | loss 0.6654
step 2820 | loss 0.3719
step 2830 | loss 0.2041
step 2840 | loss 0.3921
step 2850 | loss 0.2031
step 2860 | loss 0.2349
step 2870 | loss 0.3783
step 2880 | loss 0.3024
step 2890 | loss 0.2589
step 2900 | loss 0.3199
step 2910 | loss 0.2919
step 2920 | loss 0.4027
step 2930 | loss 0.3216
step 2940 | loss 0.2427
step 2950 | loss 0.1897
step 2960 | loss 0.1622
step 2970 | loss 0.2718
step 2980 | loss 0.4391
step 2990 | loss 0.2782
Running final evaluation...
step 3000: train loss 0.2873, val loss 0.8631
Done.
================================================================
                  RNN EXPERIMENTS COMPLETE
================================================================