rohan@jsba:~/music-scaling-laws$ docker compose exec dev python -u src/train_xxl.py     --n_layer 24     --n_embd 1600     --max_iters 10000     --batch_size 1     --grad_accum 32
WARN[0000] /home/rohan/music-scaling-laws/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion
--- Starting XXL Training Run (10000 steps) ---
Initializing Data Loaders...
 -> Loaded train data (14115371080 tokens) via MemMap.
 -> Loaded val data (40196277 tokens) via MemMap.
Initializing XXL Model: 24L, 1600D, 25H...
number of parameters: 737.94M
Parameters: 739.58M
Starting Training Loop...
Running evaluation at step 0...
step 0: train 4.8350, val 4.9583
  >> New Best! Saving checkpoint...
step 0 | loss 4.9710
step 10 | loss 4.3443
step 20 | loss 3.1699
step 30 | loss 2.9565
step 40 | loss 1.4089
step 50 | loss 0.1419
step 60 | loss 1.7589
step 70 | loss 2.2502
step 80 | loss 0.5577
step 90 | loss 0.1100
step 100 | loss 0.4871
step 110 | loss 1.1076
step 120 | loss 0.0786
step 130 | loss 0.9984
step 140 | loss 0.8084
step 150 | loss 0.0699
step 160 | loss 1.1463
step 170 | loss 0.2646
step 180 | loss 0.2033
step 190 | loss 1.6481
step 200 | loss 1.6072
step 210 | loss 1.1261
step 220 | loss 0.3140
step 230 | loss 0.2095
step 240 | loss 0.0710
Running evaluation at step 250...
step 250: train 0.9057, val 1.6165
  >> New Best! Saving checkpoint...
step 250 | loss 1.5928
step 260 | loss 0.5167
step 270 | loss 0.0829
step 280 | loss 0.9135
step 290 | loss 0.5955
step 300 | loss 1.0349
step 310 | loss 0.4730
step 320 | loss 0.2686
step 330 | loss 0.0970
step 340 | loss 0.0677
step 350 | loss 0.3301
step 360 | loss 0.2531
step 370 | loss 1.5974
step 380 | loss 0.3535
step 390 | loss 1.7828
step 400 | loss 1.0386
step 410 | loss 0.0764
step 420 | loss 0.0805
step 430 | loss 0.0784
step 440 | loss 1.0281
step 450 | loss 0.0709
step 460 | loss 0.0724
step 470 | loss 0.0713
step 480 | loss 1.7276
step 490 | loss 0.0689
Running evaluation at step 500...
step 500: train 0.5264, val 1.5113
  >> New Best! Saving checkpoint...
step 500 | loss 0.0684
step 510 | loss 0.5952
step 520 | loss 1.7744
step 530 | loss 1.7304
step 540 | loss 0.0904
step 550 | loss 0.0722
step 560 | loss 0.4462
step 570 | loss 2.0874
step 580 | loss 1.8086
step 590 | loss 0.8035
step 600 | loss 0.0713
step 610 | loss 0.9855
step 620 | loss 0.4702
step 630 | loss 0.7906
step 640 | loss 0.0901
step 650 | loss 1.6818
step 660 | loss 0.0926
step 670 | loss 0.9380
step 680 | loss 0.0700
step 690 | loss 1.3698
step 700 | loss 0.2854
step 710 | loss 0.8705
step 720 | loss 0.2665
step 730 | loss 0.0933
step 740 | loss 0.4262
Running evaluation at step 750...
step 750: train 0.4644, val 1.2801
  >> New Best! Saving checkpoint...
step 750 | loss 1.6385
step 760 | loss 0.6670
step 770 | loss 0.0728
step 780 | loss 0.2319
step 790 | loss 0.3668
step 800 | loss 0.0669
step 810 | loss 0.6693
step 820 | loss 0.6054
step 830 | loss 0.0692
step 840 | loss 0.0704
step 850 | loss 1.1463
step 860 | loss 0.6682
step 870 | loss 0.2554
step 880 | loss 1.9023
step 890 | loss 1.6565
step 900 | loss 1.3889
step 910 | loss 1.6435
step 920 | loss 0.2812
step 930 | loss 1.3339
step 940 | loss 0.1901
step 950 | loss 0.0899
step 960 | loss 0.0870
step 970 | loss 0.0842
step 980 | loss 0.2088
step 990 | loss 0.0054
Running evaluation at step 1000...
step 1000: train 0.4361, val 1.3809
step 1000 | loss 1.6161
step 1010 | loss 1.1671
step 1020 | loss 0.2000
step 1030 | loss 1.2213
step 1040 | loss 0.5737
step 1050 | loss 1.3139
step 1060 | loss 1.1049
step 1070 | loss 0.0246
step 1080 | loss 0.1557
step 1090 | loss 0.7204
step 1100 | loss 0.0047
step 1110 | loss 0.0150
step 1120 | loss 0.3156
step 1130 | loss 0.0053
step 1140 | loss 0.0182
step 1150 | loss 0.1245
step 1160 | loss 0.1232
step 1170 | loss 1.1320
step 1180 | loss 0.0886
step 1190 | loss 0.1134
step 1200 | loss 0.8155
step 1210 | loss 1.0359
step 1220 | loss 0.0977
step 1230 | loss 0.0722
step 1240 | loss 1.6260
Running evaluation at step 1250...
step 1250: train 0.5197, val 1.2503
  >> New Best! Saving checkpoint...
step 1250 | loss 0.1890
step 1260 | loss 1.3420
step 1270 | loss 0.1526
step 1280 | loss 0.0452
step 1290 | loss 0.1788
step 1300 | loss 0.0047
step 1310 | loss 1.2390
step 1320 | loss 1.3436
step 1330 | loss 0.0035
step 1340 | loss 0.1881
step 1350 | loss 0.0943
step 1360 | loss 0.0030
step 1370 | loss 0.0134
step 1380 | loss 0.2386
step 1390 | loss 0.1512
step 1400 | loss 0.1972
step 1410 | loss 1.5339
step 1420 | loss 0.3693
step 1430 | loss 0.1754
step 1440 | loss 0.0052
step 1450 | loss 0.0051
step 1460 | loss 1.2158
step 1470 | loss 1.1800
step 1480 | loss 1.1379
step 1490 | loss 0.9918
Running evaluation at step 1500...
step 1500: train 0.3843, val 1.0527
  >> New Best! Saving checkpoint...
step 1500 | loss 0.0730
step 1510 | loss 0.0101
step 1520 | loss 0.3781
step 1530 | loss 0.0077
step 1540 | loss 0.0448
step 1550 | loss 0.8905
step 1560 | loss 0.0087
step 1570 | loss 0.8934
step 1580 | loss 1.4703
step 1590 | loss 0.1215
step 1600 | loss 0.0271
step 1610 | loss 0.0778
step 1620 | loss 1.1615
step 1630 | loss 0.0046
step 1640 | loss 0.0212
step 1650 | loss 0.0688
step 1660 | loss 1.3592
step 1670 | loss 0.1625
step 1680 | loss 0.0598
step 1690 | loss 1.3865
step 1700 | loss 0.0041
step 1710 | loss 1.6775
step 1720 | loss 1.1758
step 1730 | loss 1.0500
step 1740 | loss 0.0023
Running evaluation at step 1750...
step 1750: train 0.3882, val 1.0308
  >> New Best! Saving checkpoint...
step 1750 | loss 0.0059
step 1760 | loss 1.4651
step 1770 | loss 0.0052
step 1780 | loss 0.7376
step 1790 | loss 0.0089
step 1800 | loss 0.2317
step 1810 | loss 0.4283
step 1820 | loss 1.2396
step 1830 | loss 0.2269
step 1840 | loss 0.9212
step 1850 | loss 0.1214
step 1860 | loss 0.0163
step 1870 | loss 0.0465
step 1880 | loss 0.0125
step 1890 | loss 0.9349
step 1900 | loss 0.0496
step 1910 | loss 0.0227
step 1920 | loss 0.0071
step 1930 | loss 0.0039
step 1940 | loss 1.2557
step 1950 | loss 0.0035
step 1960 | loss 0.0026
step 1970 | loss 0.0931
step 1980 | loss 0.0021
step 1990 | loss 0.8541
Running evaluation at step 2000...
step 2000: train 0.3128, val 0.8591
  >> New Best! Saving checkpoint...
step 2000 | loss 0.0525
step 2010 | loss 1.4417
step 2020 | loss 0.0653
step 2030 | loss 0.0054
step 2040 | loss 0.0126
step 2050 | loss 0.0071
step 2060 | loss 1.0581
step 2070 | loss 1.4586
step 2080 | loss 1.4191
step 2090 | loss 0.0026
step 2100 | loss 0.0646
step 2110 | loss 0.0028
step 2120 | loss 0.0033
step 2130 | loss 0.0048
step 2140 | loss 0.0091
step 2150 | loss 0.0041
step 2160 | loss 0.0022
step 2170 | loss 0.3351
step 2180 | loss 0.7768
step 2190 | loss 0.9445
step 2200 | loss 0.5225
step 2210 | loss 0.3321
step 2220 | loss 0.5152
step 2230 | loss 0.0039
step 2240 | loss 0.0053
Running evaluation at step 2250...
step 2250: train 0.3082, val 0.8261
  >> New Best! Saving checkpoint...
step 2250 | loss 0.0105
step 2260 | loss 0.0036
step 2270 | loss 0.2147
step 2280 | loss 0.6649
step 2290 | loss 0.4477
step 2300 | loss 0.0133
step 2310 | loss 0.2894
step 2320 | loss 0.6952
step 2330 | loss 0.5060
step 2340 | loss 0.0265
step 2350 | loss 0.0018
step 2360 | loss 0.0243
step 2370 | loss 0.1436
step 2380 | loss 0.0037
step 2390 | loss 0.0036
step 2400 | loss 0.0014
step 2410 | loss 0.0310
step 2420 | loss 0.0015
step 2430 | loss 0.0214
step 2440 | loss 0.0238
step 2450 | loss 0.0044
step 2460 | loss 0.0024
step 2470 | loss 0.0035
step 2480 | loss 0.0017
step 2490 | loss 0.0050
Running evaluation at step 2500...
step 2500: train 0.1888, val 0.6710
  >> New Best! Saving checkpoint...
step 2500 | loss 0.8586
step 2510 | loss 0.0108
step 2520 | loss 0.0229
step 2530 | loss 0.8959
step 2540 | loss 0.0307
step 2550 | loss 0.0735
step 2560 | loss 0.0067
step 2570 | loss 0.0062
step 2580 | loss 0.0050
step 2590 | loss 0.0058
step 2600 | loss 1.1573
step 2610 | loss 0.0154
step 2620 | loss 0.0016
step 2630 | loss 0.0019
step 2640 | loss 0.0055
step 2650 | loss 0.5941
step 2660 | loss 0.0024
step 2670 | loss 0.0022
step 2680 | loss 0.0025
step 2690 | loss 0.0027
step 2700 | loss 0.3712
step 2710 | loss 0.0021
step 2720 | loss 0.4954
step 2730 | loss 0.0020
step 2740 | loss 0.5889
Running evaluation at step 2750...
step 2750: train 0.2503, val 0.6587
  >> New Best! Saving checkpoint...
step 2750 | loss 0.6882
step 2760 | loss 0.0031
step 2770 | loss 0.4740
step 2780 | loss 0.0024
step 2790 | loss 0.0075
step 2800 | loss 0.0132
step 2810 | loss 0.8159
step 2820 | loss 0.1170
step 2830 | loss 0.0107
step 2840 | loss 0.5265
step 2850 | loss 0.8137
step 2860 | loss 0.0022
step 2870 | loss 0.6454
step 2880 | loss 1.1672
step 2890 | loss 1.1237
step 2900 | loss 0.2300
step 2910 | loss 0.0028
step 2920 | loss 0.0096
step 2930 | loss 0.0032
step 2940 | loss 0.6251
step 2950 | loss 0.0026
step 2960 | loss 0.0152
step 2970 | loss 0.7474
step 2980 | loss 0.8685
step 2990 | loss 0.0091
Running evaluation at step 3000...
step 3000: train 0.1668, val 0.5697
  >> New Best! Saving checkpoint...
step 3000 | loss 0.0120
step 3010 | loss 0.0024
step 3020 | loss 0.0031
step 3030 | loss 0.0084
step 3040 | loss 0.1174
step 3050 | loss 0.0040
step 3060 | loss 0.0114
step 3070 | loss 0.2288
step 3080 | loss 0.0792
step 3090 | loss 0.0112
step 3100 | loss 0.0021
step 3110 | loss 0.0178
step 3120 | loss 0.0082
step 3130 | loss 0.0127
step 3140 | loss 0.7943
step 3150 | loss 0.1411
step 3160 | loss 0.6674
step 3170 | loss 0.0090
step 3180 | loss 0.0617
step 3190 | loss 0.0066
step 3200 | loss 0.0021
step 3210 | loss 0.1089
step 3220 | loss 0.0058
step 3230 | loss 0.5356
step 3240 | loss 0.2655
Running evaluation at step 3250...
step 3250: train 0.1569, val 0.5795
step 3250 | loss 0.3351
step 3260 | loss 0.0024
step 3270 | loss 0.0016
step 3280 | loss 0.5758
step 3290 | loss 0.0018
step 3300 | loss 0.2689
step 3310 | loss 0.0029
step 3320 | loss 0.0053
step 3330 | loss 0.0059
step 3340 | loss 0.0715
step 3350 | loss 0.6706
step 3360 | loss 0.0106
step 3370 | loss 0.0063
step 3380 | loss 0.0054
step 3390 | loss 0.0025
step 3400 | loss 0.0236
step 3410 | loss 0.0011
step 3420 | loss 0.0026
step 3430 | loss 0.1108
step 3440 | loss 0.0019
step 3450 | loss 0.0063
step 3460 | loss 0.0025
step 3470 | loss 0.0120
step 3480 | loss 0.0052
step 3490 | loss 0.1327
Running evaluation at step 3500...
step 3500: train 0.1089, val 0.5235
  >> New Best! Saving checkpoint...
step 3500 | loss 0.0132
step 3510 | loss 1.0348
step 3520 | loss 0.0860
step 3530 | loss 0.6006
step 3540 | loss 0.0070
step 3550 | loss 0.4330
step 3560 | loss 0.0034
step 3570 | loss 0.0085
step 3580 | loss 0.0050
step 3590 | loss 0.9957
step 3600 | loss 0.0027
step 3610 | loss 0.0017
step 3620 | loss 0.0105
step 3630 | loss 0.0043
step 3640 | loss 0.0017
step 3650 | loss 0.1328
step 3660 | loss 0.0143
step 3670 | loss 0.0079
step 3680 | loss 0.5115
step 3690 | loss 0.0027
step 3700 | loss 0.0029
step 3710 | loss 0.0033
step 3720 | loss 0.0024
step 3730 | loss 0.0016
step 3740 | loss 0.8486
Running evaluation at step 3750...
step 3750: train 0.1648, val 0.4625
  >> New Best! Saving checkpoint...
step 3750 | loss 0.2107
step 3760 | loss 0.3386
step 3770 | loss 0.0041
step 3780 | loss 0.0140
step 3790 | loss 0.5704
step 3800 | loss 0.2053
step 3810 | loss 0.0079
step 3820 | loss 0.0023
step 3830 | loss 0.0037
step 3840 | loss 0.0946
step 3850 | loss 0.0037
step 3860 | loss 0.0555
step 3870 | loss 0.1655
step 3880 | loss 0.7002
step 3890 | loss 0.8643
step 3900 | loss 0.0041
step 3910 | loss 0.4205
step 3920 | loss 0.0026
step 3930 | loss 0.0025
step 3940 | loss 0.0070
step 3950 | loss 1.0212
step 3960 | loss 0.4374
step 3970 | loss 0.0084
step 3980 | loss 0.0144
step 3990 | loss 0.0019
Running evaluation at step 4000...
step 4000: train 0.1372, val 0.4361
  >> New Best! Saving checkpoint...
step 4000 | loss 0.0744
step 4010 | loss 0.0014
step 4020 | loss 0.1726
step 4030 | loss 0.0062
step 4040 | loss 0.0049
step 4050 | loss 0.0047
step 4060 | loss 0.0022
step 4070 | loss 0.0028
step 4080 | loss 0.2886
step 4090 | loss 0.0024
step 4100 | loss 0.0013
step 4110 | loss 0.0024
step 4120 | loss 0.0091
step 4130 | loss 0.0086
step 4140 | loss 0.0039
step 4150 | loss 0.0125
step 4160 | loss 0.0010
step 4170 | loss 0.0761
step 4180 | loss 0.9629
step 4190 | loss 0.0024
step 4200 | loss 0.0028
step 4210 | loss 0.0043
step 4220 | loss 0.5672
step 4230 | loss 0.2535
step 4240 | loss 0.0058
Running evaluation at step 4250...
step 4250: train 0.0997, val 0.3651
  >> New Best! Saving checkpoint...
step 4250 | loss 0.0092
step 4260 | loss 0.0032
step 4270 | loss 0.4321
step 4280 | loss 0.0914
step 4290 | loss 0.0090
step 4300 | loss 0.0296
step 4310 | loss 0.0021
step 4320 | loss 0.3305
step 4330 | loss 0.0235
step 4340 | loss 0.3945
step 4350 | loss 0.0026
step 4360 | loss 0.0065
step 4370 | loss 0.1078
step 4380 | loss 0.0114
step 4390 | loss 0.0090
step 4400 | loss 0.2969
step 4410 | loss 0.0029
step 4420 | loss 0.2339
step 4430 | loss 0.0142
step 4440 | loss 0.0022
step 4450 | loss 0.7711
step 4460 | loss 0.0048
step 4470 | loss 0.0058
step 4480 | loss 0.0078
step 4490 | loss 0.0581
Running evaluation at step 4500...
step 4500: train 0.1835, val 0.3844
step 4500 | loss 0.3268
step 4510 | loss 0.0044
step 4520 | loss 0.0027
step 4530 | loss 0.0800
step 4540 | loss 0.0027
step 4550 | loss 0.0614
step 4560 | loss 0.1412
step 4570 | loss 0.3812
step 4580 | loss 0.0017
step 4590 | loss 0.0988
step 4600 | loss 0.0055
step 4610 | loss 0.0029
step 4620 | loss 0.1983
step 4630 | loss 0.0019
step 4640 | loss 0.1719
step 4650 | loss 0.0125
step 4660 | loss 0.0066
step 4670 | loss 0.0014
step 4680 | loss 0.0032
step 4690 | loss 0.0090
step 4700 | loss 0.0058
step 4710 | loss 0.0029
step 4720 | loss 0.0018
step 4730 | loss 0.3478
step 4740 | loss 0.6016
Running evaluation at step 4750...
step 4750: train 0.0712, val 0.3661
step 4750 | loss 0.0025
step 4760 | loss 0.0037
step 4770 | loss 0.0041
step 4780 | loss 0.0027
step 4790 | loss 0.0051
step 4800 | loss 0.0038
step 4810 | loss 0.0020
step 4820 | loss 0.0028
step 4830 | loss 0.2552
step 4840 | loss 0.0071
step 4850 | loss 0.5838
step 4860 | loss 0.0030
step 4870 | loss 0.0047
step 4880 | loss 0.0024
step 4890 | loss 0.0038
step 4900 | loss 0.0043
step 4910 | loss 0.0102
step 4920 | loss 0.9393
step 4930 | loss 0.1762
step 4940 | loss 0.0031
step 4950 | loss 0.0071
step 4960 | loss 0.0079
step 4970 | loss 0.0054
step 4980 | loss 0.0593
step 4990 | loss 0.0044
Running evaluation at step 5000...
step 5000: train 0.1380, val 0.3606
  >> New Best! Saving checkpoint...
step 5000 | loss 0.0033
step 5010 | loss 0.0095
step 5020 | loss 0.6121
step 5030 | loss 0.3292
step 5040 | loss 0.0023
step 5050 | loss 0.6671
step 5060 | loss 0.5287
step 5070 | loss 0.0025
step 5080 | loss 0.4148
step 5090 | loss 0.0027
step 5100 | loss 0.4131
step 5110 | loss 0.0021
step 5120 | loss 0.7589
step 5130 | loss 0.6563
step 5140 | loss 0.0043
step 5150 | loss 0.0074
step 5160 | loss 0.0075
step 5170 | loss 0.6732
step 5180 | loss 0.0055
step 5190 | loss 0.0070
step 5200 | loss 0.2926
step 5210 | loss 0.0031
step 5220 | loss 0.3586
step 5230 | loss 0.0017
step 5240 | loss 0.0028
Running evaluation at step 5250...
step 5250: train 0.1368, val 0.3704
step 5250 | loss 0.0024
step 5260 | loss 0.0057
step 5270 | loss 0.0019
step 5280 | loss 0.0029
step 5290 | loss 0.0354
step 5300 | loss 0.0033
step 5310 | loss 0.1184
step 5320 | loss 0.0134
step 5330 | loss 0.0124
step 5340 | loss 0.0018
step 5350 | loss 0.0020
step 5360 | loss 0.0024
step 5370 | loss 0.0114
step 5380 | loss 0.0050
step 5390 | loss 0.0071
step 5400 | loss 0.3856
step 5410 | loss 0.0639
step 5420 | loss 0.0558
step 5430 | loss 0.0041
step 5440 | loss 0.3774
step 5450 | loss 0.0067
step 5460 | loss 0.0311
step 5470 | loss 0.0064
step 5480 | loss 0.0607
step 5490 | loss 0.0023
Running evaluation at step 5500...
step 5500: train 0.1077, val 0.4024
step 5500 | loss 0.3738
step 5510 | loss 0.0014
step 5520 | loss 0.0086
step 5530 | loss 0.1089
step 5540 | loss 0.0025
step 5550 | loss 0.0094
step 5560 | loss 0.0024
step 5570 | loss 0.0039
step 5580 | loss 0.0017
step 5590 | loss 0.0040
step 5600 | loss 0.0025
step 5610 | loss 0.1254
step 5620 | loss 0.0032
step 5630 | loss 0.0042
step 5640 | loss 0.0029
step 5650 | loss 0.5031
step 5660 | loss 0.6735
step 5670 | loss 0.0017
step 5680 | loss 0.0042
step 5690 | loss 0.1925
step 5700 | loss 0.0042
step 5710 | loss 0.0054
step 5720 | loss 0.0024
step 5730 | loss 0.2898
step 5740 | loss 0.0924
Running evaluation at step 5750...
step 5750: train 0.0862, val 0.4762
step 5750 | loss 0.0137
step 5760 | loss 0.0039
step 5770 | loss 0.0116
step 5780 | loss 0.0030
step 5790 | loss 0.0022
step 5800 | loss 0.0068
step 5810 | loss 0.3596
step 5820 | loss 0.0103
step 5830 | loss 0.0111
step 5840 | loss 0.0302
step 5850 | loss 0.0061
step 5860 | loss 0.0022
step 5870 | loss 0.0038
step 5880 | loss 0.0075
step 5890 | loss 0.0057
step 5900 | loss 0.1778
step 5910 | loss 0.0017
step 5920 | loss 0.0557
step 5930 | loss 0.1753
step 5940 | loss 0.0027
step 5950 | loss 0.3778
step 5960 | loss 0.0039
step 5970 | loss 0.0035
step 5980 | loss 0.0035
step 5990 | loss 0.5768
Running evaluation at step 6000...
step 6000: train 0.1064, val 0.4197
step 6000 | loss 0.2796
step 6010 | loss 0.0049
step 6020 | loss 0.0104
step 6030 | loss 0.0033
step 6040 | loss 0.0030
step 6050 | loss 0.2498
step 6060 | loss 0.0987
step 6070 | loss 0.0023
step 6080 | loss 0.0284
step 6090 | loss 0.0494
step 6100 | loss 0.0039
step 6110 | loss 0.0688
step 6120 | loss 0.0657
step 6130 | loss 0.4736
step 6140 | loss 0.0024
step 6150 | loss 0.0077
step 6160 | loss 0.6876
step 6170 | loss 0.0042
step 6180 | loss 0.0036
step 6190 | loss 0.0065
step 6200 | loss 0.0089
step 6210 | loss 0.1258
step 6220 | loss 0.7356
step 6230 | loss 0.0023
step 6240 | loss 0.0037
Running evaluation at step 6250...
step 6250: train 0.1454, val 0.4238
step 6250 | loss 0.1120
step 6260 | loss 0.0051
step 6270 | loss 0.5722
step 6280 | loss 0.0049
step 6290 | loss 0.0537
step 6300 | loss 0.0977
step 6310 | loss 0.2075
step 6320 | loss 0.3277
step 6330 | loss 0.0028
step 6340 | loss 0.0020
step 6350 | loss 0.1852
step 6360 | loss 0.5350
step 6370 | loss 0.0025
step 6380 | loss 0.4006
step 6390 | loss 0.0871
step 6400 | loss 0.0070
step 6410 | loss 0.0067
step 6420 | loss 0.0033
step 6430 | loss 0.0023
step 6440 | loss 0.0041
step 6450 | loss 0.9328
step 6460 | loss 0.1914
step 6470 | loss 0.0037
step 6480 | loss 0.0034
step 6490 | loss 0.0023
Running evaluation at step 6500...
step 6500: train 0.0966, val 0.3351
  >> New Best! Saving checkpoint...
step 6500 | loss 0.2357
step 6510 | loss 0.0044
step 6520 | loss 0.0049
step 6530 | loss 0.0476
step 6540 | loss 0.2096
step 6550 | loss 0.0021
step 6560 | loss 0.5345
step 6570 | loss 0.0053
step 6580 | loss 0.0013
step 6590 | loss 0.0014
step 6600 | loss 0.0498
step 6610 | loss 0.0352
step 6620 | loss 0.0540
step 6630 | loss 0.0636
step 6640 | loss 0.0024
step 6650 | loss 0.2836
step 6660 | loss 0.3350
step 6670 | loss 0.0048
step 6680 | loss 0.0076
step 6690 | loss 0.2260
step 6700 | loss 0.0027
step 6710 | loss 0.0046
step 6720 | loss 0.0045
step 6730 | loss 0.0019
step 6740 | loss 0.0420
Running evaluation at step 6750...
step 6750: train 0.1259, val 0.3496
step 6750 | loss 0.0081
step 6760 | loss 0.2474
step 6770 | loss 0.3386
step 6780 | loss 0.0382
step 6790 | loss 0.0027
step 6800 | loss 0.0863
step 6810 | loss 0.0033
step 6820 | loss 0.2931
step 6830 | loss 0.0097
step 6840 | loss 0.0032
step 6850 | loss 0.0024
step 6860 | loss 0.6279
step 6870 | loss 0.9419
step 6880 | loss 0.0014
step 6890 | loss 0.0158
step 6900 | loss 0.3444
step 6910 | loss 0.0357
step 6920 | loss 0.0043
step 6930 | loss 0.0047
step 6940 | loss 0.0048
step 6950 | loss 0.3730
step 6960 | loss 0.0960
step 6970 | loss 0.7385
step 6980 | loss 0.5113
step 6990 | loss 0.2355
Running evaluation at step 7000...
step 7000: train 0.1072, val 0.3814
step 7000 | loss 1.0854
step 7010 | loss 0.0046
step 7020 | loss 0.0086
step 7030 | loss 0.0014
step 7040 | loss 0.0072
step 7050 | loss 0.0029
step 7060 | loss 0.0019
step 7070 | loss 0.1874
step 7080 | loss 0.7440
step 7090 | loss 0.0027
step 7100 | loss 0.0030
step 7110 | loss 0.0085
step 7120 | loss 0.0131
step 7130 | loss 0.7207
step 7140 | loss 0.0723
step 7150 | loss 0.3056
step 7160 | loss 0.7438
step 7170 | loss 0.2473
step 7180 | loss 0.0025
step 7190 | loss 0.0062
step 7200 | loss 0.0019
step 7210 | loss 0.1347
step 7220 | loss 0.0026
step 7230 | loss 0.0162
step 7240 | loss 0.0044
Running evaluation at step 7250...
step 7250: train 0.0754, val 0.3175
  >> New Best! Saving checkpoint...
step 7250 | loss 0.0162
step 7260 | loss 0.6228
step 7270 | loss 0.0061
step 7280 | loss 0.0452
step 7290 | loss 0.0724
step 7300 | loss 0.1610
step 7310 | loss 0.0544
step 7320 | loss 0.0051
step 7330 | loss 0.0015
step 7340 | loss 0.0982
step 7350 | loss 0.4359
step 7360 | loss 0.0573
step 7370 | loss 0.4047
step 7380 | loss 0.4421
step 7390 | loss 0.0016
step 7400 | loss 0.0385
step 7410 | loss 0.0548
step 7420 | loss 0.4075
step 7430 | loss 0.0022
step 7440 | loss 0.2224
step 7450 | loss 0.0305
step 7460 | loss 1.3349
step 7470 | loss 0.0027
step 7480 | loss 0.0932
step 7490 | loss 0.0025
Running evaluation at step 7500...
step 7500: train 0.1026, val 0.3616
step 7500 | loss 0.0567
step 7510 | loss 0.0052
step 7520 | loss 0.4019
step 7530 | loss 0.0066
step 7540 | loss 0.0453
step 7550 | loss 0.4621
step 7560 | loss 0.0059
step 7570 | loss 0.0078
step 7580 | loss 0.0026
step 7590 | loss 0.0081
step 7600 | loss 0.0102
step 7610 | loss 0.0076
step 7620 | loss 0.0063
step 7630 | loss 0.1062
step 7640 | loss 0.0025
step 7650 | loss 0.0083
step 7660 | loss 0.0576
step 7670 | loss 0.0033
step 7680 | loss 0.0025
step 7690 | loss 0.0078
step 7700 | loss 0.5420
step 7710 | loss 0.0034
step 7720 | loss 0.0621
step 7730 | loss 0.3047
step 7740 | loss 0.0481
Running evaluation at step 7750...
step 7750: train 0.1384, val 0.3109
  >> New Best! Saving checkpoint...
step 7750 | loss 0.0017
step 7760 | loss 0.0339
step 7770 | loss 0.0036
step 7780 | loss 0.0299
step 7790 | loss 0.0056
step 7800 | loss 0.3007
step 7810 | loss 0.0048
step 7820 | loss 0.0097
step 7830 | loss 0.0406
step 7840 | loss 0.4748
step 7850 | loss 0.0241
step 7860 | loss 0.0022
step 7870 | loss 0.0543
step 7880 | loss 0.2123
step 7890 | loss 0.0074
step 7900 | loss 0.0025
step 7910 | loss 0.3364
step 7920 | loss 0.0260
step 7930 | loss 0.0023
step 7940 | loss 0.7476
step 7950 | loss 0.2078
step 7960 | loss 0.0022
step 7970 | loss 0.5651
step 7980 | loss 0.4843
step 7990 | loss 0.3855
Running evaluation at step 8000...
step 8000: train 0.0563, val 0.3783
step 8000 | loss 0.0020
step 8010 | loss 0.0039
step 8020 | loss 0.2464
step 8030 | loss 0.0016
step 8040 | loss 0.0041
step 8050 | loss 0.0041
step 8060 | loss 0.0023
step 8070 | loss 0.0036
step 8080 | loss 0.0204
step 8090 | loss 0.0027
step 8100 | loss 0.3139
step 8110 | loss 0.1794
step 8120 | loss 0.0078
step 8130 | loss 0.3018
step 8140 | loss 0.2005
step 8150 | loss 0.0051
step 8160 | loss 0.0046
step 8170 | loss 0.0022
step 8180 | loss 0.0773
step 8190 | loss 0.1922
step 8200 | loss 0.0028
step 8210 | loss 0.0026
step 8220 | loss 0.0025
step 8230 | loss 0.0093
step 8240 | loss 0.6020
Running evaluation at step 8250...
step 8250: train 0.1094, val 0.3806
step 8250 | loss 0.4817
step 8260 | loss 0.0014
step 8270 | loss 0.0053
step 8280 | loss 0.0078
step 8290 | loss 0.2771
step 8300 | loss 0.0042
step 8310 | loss 1.1746
step 8320 | loss 0.0579
step 8330 | loss 0.0024
step 8340 | loss 0.0313
step 8350 | loss 0.3368
step 8360 | loss 0.1782
step 8370 | loss 0.7314
step 8380 | loss 0.0036
step 8390 | loss 0.0025
step 8400 | loss 0.0557
step 8410 | loss 0.2650
step 8420 | loss 0.0088
step 8430 | loss 0.0089
step 8440 | loss 0.1896
step 8450 | loss 0.0979
step 8460 | loss 0.0039
step 8470 | loss 0.0430
step 8480 | loss 0.0088
step 8490 | loss 0.4891
Running evaluation at step 8500...
step 8500: train 0.0776, val 0.3551
step 8500 | loss 0.0062
step 8510 | loss 0.3703
step 8520 | loss 0.0658
step 8530 | loss 0.0057
step 8540 | loss 0.0021
step 8550 | loss 0.0022
step 8560 | loss 0.1758
step 8570 | loss 0.0018
step 8580 | loss 0.0024
step 8590 | loss 0.0023
step 8600 | loss 0.0095
step 8610 | loss 0.0098
step 8620 | loss 0.0016
step 8630 | loss 0.0082
step 8640 | loss 0.0090
step 8650 | loss 0.0083
step 8660 | loss 0.0080
step 8670 | loss 0.0034
step 8680 | loss 0.0097
step 8690 | loss 0.0022
step 8700 | loss 0.0037
step 8710 | loss 0.2796
step 8720 | loss 0.0064
step 8730 | loss 0.0112
step 8740 | loss 0.0019
Running evaluation at step 8750...
step 8750: train 0.1087, val 0.3983
step 8750 | loss 0.0020
step 8760 | loss 0.0074
step 8770 | loss 0.0327
step 8780 | loss 0.0034
step 8790 | loss 0.0027
step 8800 | loss 0.2543
step 8810 | loss 0.0066
step 8820 | loss 0.4621
step 8830 | loss 0.0047
step 8840 | loss 0.0055
step 8850 | loss 0.0019
step 8860 | loss 0.0072
step 8870 | loss 0.2181
step 8880 | loss 0.0022
step 8890 | loss 0.0045
step 8900 | loss 0.0264
step 8910 | loss 0.0040
step 8920 | loss 0.0063
step 8930 | loss 0.0080
step 8940 | loss 0.0030
step 8950 | loss 0.3822
step 8960 | loss 0.0026
step 8970 | loss 0.0027
step 8980 | loss 0.0009
step 8990 | loss 0.2048
Running evaluation at step 9000...
step 9000: train 0.0990, val 0.3707
step 9000 | loss 0.0022
step 9010 | loss 0.0052
step 9020 | loss 0.0464
step 9030 | loss 0.0146
step 9040 | loss 0.1633
step 9050 | loss 0.0057
step 9060 | loss 0.0050
step 9070 | loss 0.1076
step 9080 | loss 0.0016
step 9090 | loss 0.0036
step 9100 | loss 0.0558
step 9110 | loss 0.0024
step 9120 | loss 0.0409
step 9130 | loss 0.4538
step 9140 | loss 0.0389
step 9150 | loss 0.1937
step 9160 | loss 0.2925
step 9170 | loss 0.6655
step 9180 | loss 0.0167
step 9190 | loss 0.0050
step 9200 | loss 0.0093
step 9210 | loss 0.0080
step 9220 | loss 0.0026
step 9230 | loss 0.0025
step 9240 | loss 0.0034
Running evaluation at step 9250...
step 9250: train 0.1047, val 0.3202
step 9250 | loss 0.0123
step 9260 | loss 0.0016
step 9270 | loss 0.0015
step 9280 | loss 0.6402
step 9290 | loss 0.0039
step 9300 | loss 0.3205
step 9310 | loss 0.4728
step 9320 | loss 0.0038
step 9330 | loss 0.7065
step 9340 | loss 0.0017
step 9350 | loss 0.2530
step 9360 | loss 0.0026
step 9370 | loss 0.0037
step 9380 | loss 0.1342
step 9390 | loss 0.0684
step 9400 | loss 0.0027
step 9410 | loss 0.1264
step 9420 | loss 0.0037
step 9430 | loss 0.0131
step 9440 | loss 0.1947
step 9450 | loss 0.3142
step 9460 | loss 0.0078
step 9470 | loss 0.0018
step 9480 | loss 0.2990
step 9490 | loss 0.0042
Running evaluation at step 9500...
step 9500: train 0.0867, val 0.2807
  >> New Best! Saving checkpoint...
step 9500 | loss 0.3766
step 9510 | loss 0.2697
step 9520 | loss 0.0020
step 9530 | loss 0.6843
step 9540 | loss 0.0030
step 9550 | loss 0.5002
step 9560 | loss 0.4307
step 9570 | loss 0.0024
step 9580 | loss 0.0049
step 9590 | loss 0.0018
step 9600 | loss 0.0390
step 9610 | loss 0.2599
step 9620 | loss 0.0054
step 9630 | loss 0.0072
step 9640 | loss 0.3356
step 9650 | loss 0.0027
step 9660 | loss 0.0311
step 9670 | loss 0.0061
step 9680 | loss 0.0033
step 9690 | loss 0.0027
step 9700 | loss 0.1083
step 9710 | loss 0.0058
step 9720 | loss 0.0083
step 9730 | loss 0.0026
step 9740 | loss 0.0049
Running evaluation at step 9750...
step 9750: train 0.0955, val 0.3781
step 9750 | loss 0.0021
step 9760 | loss 0.0720
step 9770 | loss 0.0061
step 9780 | loss 0.0034
step 9790 | loss 0.0026
step 9800 | loss 0.0026
step 9810 | loss 0.0018
step 9820 | loss 0.1646
step 9830 | loss 0.1572
step 9840 | loss 0.0044
step 9850 | loss 0.0081
step 9860 | loss 0.0039
step 9870 | loss 0.2419
step 9880 | loss 0.0020
step 9890 | loss 0.6999
step 9900 | loss 0.0023
step 9910 | loss 0.0967
step 9920 | loss 0.5606
step 9930 | loss 0.4817
step 9940 | loss 0.0024
step 9950 | loss 0.2244
step 9960 | loss 0.0024
step 9970 | loss 0.0029
step 9980 | loss 0.0917
step 9990 | loss 0.0079
Running evaluation at step 10000...
step 10000: train 0.1087, val 0.3983
Done.